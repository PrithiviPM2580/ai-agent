import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { createAgent, trimMessages, createMiddleware } from "langchain";
import { MemorySaver } from "@langchain/langgraph";
import config from "../../config/env.config";

//
// ğŸ’¾ Memory checkpoint (short-term memory)
//
const checkpointer = new MemorySaver();

//
// ğŸ§© Trim Middleware
//
const trimMiddleware = createMiddleware({
  name: "TrimLogger",
  beforeModel: async (state) => {
    const originalCount = state.messages.length;

    // Trim the messages
    const trimmed = await trimMessages(state.messages, {
      strategy: "last",
      maxTokens: 5, // here we simulate "token" count by message count for demo
      startOn: "human",
      endOn: ["human", "tool"],
      tokenCounter: (msgs) => msgs.length, // fake token counter
    });

    const trimmedCount = trimmed.length;

    console.log("\nğŸ“‹ --- TRIM CHECK ---");
    console.log("ğŸ§¾ Original message count:", originalCount);
    console.log("âœ‚ï¸ Trimmed message count:", trimmedCount);

    if (trimmedCount < originalCount) {
      console.log("âœ… Trimmed messages:");
      const removed = state.messages.slice(0, originalCount - trimmedCount);
      removed.forEach((m, i) =>
        console.log(`ğŸ—‘ï¸ Removed [${i}]: (${m.type}) ${m.content}`)
      );
    } else {
      console.log("â­ï¸ No trimming needed.");
    }

    console.log(
      "ğŸ§© Messages sent to model:",
      trimmed.map((m) => m.content)
    );
    console.log("------------------------\n");

    return { ...state, messages: trimmed };
  },
});

//
// ğŸ§  Model
//
const model = new ChatGoogleGenerativeAI({
  model: config.MODEL,
  apiKey: config.GEMINI_API_KEY,
  temperature: 0.7,
  maxOutputTokens: 1024,
});

//
// ğŸ¤– Agent
//
const agent = createAgent({
  model,
  tools: [],
  middleware: [trimMiddleware],
  checkpointer,
});

//
// ğŸ’¬ Simulate a long chat
//
const conversation = [
  { role: "user", content: "Hi!" },
  { role: "ai", content: "Hello!" },
  { role: "user", content: "Tell me a story about a dragon." },
  {
    role: "ai",
    content: "Once upon a time, there was a dragon who loved pizza.",
  },
  { role: "user", content: "What color was the dragon?" },
  { role: "ai", content: "It was a shiny golden dragon." },
  { role: "user", content: "Who was his best friend?" },
];

//
// ğŸš€ Run with a thread id
//

(async () => {
  const res = await agent.invoke(
    { messages: conversation },
    { configurable: { thread_id: "thread_1" } }
  );

  console.log("ğŸ¤– Final model response:", res.messages.at(-1)?.content);
})();
